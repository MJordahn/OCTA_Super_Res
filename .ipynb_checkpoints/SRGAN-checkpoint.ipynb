{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS TO BE USED DURING TRAINING\n",
    "import DataLoader\n",
    "from DataLoader import MabulaDataset\n",
    "from albumentations import Flip, Rotate, RandomCrop, Blur\n",
    "from albumentations.pytorch import ToTensor\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "##########################################################\n",
    "############### Model creation ###########################\n",
    "\n",
    "from SRResNetBlock import *\n",
    "# Creates the model to be used.\n",
    "def create_model(dim=320, scale_factor=2, batch_size=1, n_res_blocks=8, GPU=True):   \n",
    "    t = transformationMatrix(dim, scale_factor, batch_size, GPU=GPU)[0,0]\n",
    "    t_test = transformationMatrix(320, scale_factor, batch_size, GPU=GPU)[0,0]\n",
    "    net = SRResNet(scale_factor=scale_factor, n_res_blocks=n_res_blocks)\n",
    "    discriminator = SRResNetDiscriminator()\n",
    "    #discriminator = Discriminator()\n",
    "    # move models to GPU, if available\n",
    "    if torch.cuda.is_available() and GPU:\n",
    "        net.to(torch.device(\"cuda:0\"))\n",
    "        discriminator.to(torch.device(\"cuda:0\"))\n",
    "        print('Models moved to GPU.')\n",
    "    else:\n",
    "        print('Only CPU available.')\n",
    "    return net, discriminator, t, t_test\n",
    "\n",
    "##########################################################\n",
    "############### Losses ###################################\n",
    "\n",
    "def real_mse_loss(D_out):\n",
    "    # how close is D_out from being \"real\"?\n",
    "    l=0\n",
    "    for each in D_out:\n",
    "        l += torch.mean((each-1.0)**2)\n",
    "    return l\n",
    "\n",
    "def fake_mse_loss(D_out):\n",
    "    # how close is D_out from being \"fake\"?\n",
    "    l=0\n",
    "    for each in D_out:\n",
    "        l += torch.mean((each)**2)\n",
    "    return l\n",
    "\n",
    "\n",
    "##########################################################\n",
    "############### Training loop ############################\n",
    "\n",
    "from Utility import *\n",
    "def training_loop(models, optimizers, dataloaders,  t, t_test, n_epochs=200, GAN_loss_weight=10**-3, GPU=True):\n",
    "    generator, discriminator = models\n",
    "    g_optimizer, d_optimizer = optimizers\n",
    "    train_loader, test_loader = dataloaders\n",
    "\n",
    "    print_every= 10\n",
    "    test_every = 20\n",
    "    \n",
    "    # keep track of losses over time\n",
    "    losses = []\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "    meanPSNR_lst = []\n",
    "    meanSSIM_lst = []\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        iter_train = iter(train_loader)\n",
    "        for batch in train_loader:\n",
    "            X_hr_discriminator = iter_train.next()['image']\n",
    "            X_hr_true = batch['image']\n",
    "            if torch.cuda.is_available() and GPU:\n",
    "                X_hr_discriminator = X_hr_discriminator.to(\"cuda:0\")\n",
    "                X_hr_true = X_hr_true.to(\"cuda:0\")\n",
    "            X_lr = torch.matmul(t, X_hr_true)\n",
    "            # ============================================\n",
    "            #            TRAIN THE DISCRIMINATOR\n",
    "            d_optimizer.zero_grad()\n",
    "            # 1. Compute the REAL loss on REAL the images:\n",
    "            D_X_hr_real = discriminator(X_hr_discriminator)\n",
    "            D_X_hr_real_loss = real_mse_loss(D_X_hr_real)\n",
    "            # 2. Generate fake images high resolution images:\n",
    "            X_hr_fake = generator(X_lr)\n",
    "            # 3. Compute the FAKE loss for the FAKE image:\n",
    "            D_X_hr_fake = discriminator(X_hr_fake)\n",
    "            D_X_hr_fake_loss = fake_mse_loss(D_X_hr_fake)\n",
    "            # 4. Compute the total loss and perform backprop:\n",
    "            d_x_loss = D_X_hr_real_loss + D_X_hr_fake_loss\n",
    "            d_x_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            # =========================================\n",
    "            #            TRAIN THE GENERATOR\n",
    "            g_optimizer.zero_grad()\n",
    "            # 1. Generate fake images high resolution images:\n",
    "            X_hr_fake = generator(X_lr)\n",
    "            # 2. Compute the REAL loss based on the FAKE image:\n",
    "            D_X_hr_fake = discriminator(X_hr_fake)\n",
    "            D_X_hr_real_fake_loss = real_mse_loss(D_X_hr_fake)\n",
    "            # 3. Compute the MSE loss:\n",
    "            MSE_loss = nn.MSELoss()(X_hr_fake, X_hr_true)\n",
    "            # 4. Compute the total loss and perform backprop:\n",
    "            g_T_real_loss = GAN_loss_weight*D_X_hr_real_fake_loss + MSE_loss\n",
    "            g_T_real_loss.backward()\n",
    "            g_optimizer.step()\n",
    "        \n",
    "        has_tested = False\n",
    "        if epoch % test_every == 0:\n",
    "            print(\"#### Testing \", epoch, \" ####\")\n",
    "            generator.eval()\n",
    "            PSNR_lst = []\n",
    "            SSIM_lst = []\n",
    "            for batch in test_loader:\n",
    "                X_hr_true = batch['image']\n",
    "                if torch.cuda.is_available() and GPU:\n",
    "                    X_hr_true = X_hr_true.to(\"cuda:0\")\n",
    "                X_lr = torch.matmul(t_test, X_hr_true)\n",
    "                with torch.no_grad():\n",
    "                    X_hr_fake = generator(X_lr)\n",
    "                X_hr_fake = X_hr_fake.cpu()\n",
    "                X_hr_true =  X_hr_true.cpu()\n",
    "                PSNR, SSIM = calculate_scores(X_hr_fake, X_hr_true)\n",
    "                PSNR_lst.append(PSNR)\n",
    "                SSIM_lst.append(SSIM)\n",
    "            meanPSNR = np.mean(PSNR_lst)\n",
    "            meanSSIM = np.mean(SSIM_lst)\n",
    "            meanPSNR_lst.append(meanPSNR)\n",
    "            meanSSIM_lst.append(meanSSIM)\n",
    "            generator.train()\n",
    "            has_tested = True\n",
    "        \n",
    "        # Logging info\n",
    "        if epoch % print_every == 0 and has_tested:\n",
    "            print(\"---- Epoch nr: \", epoch, \" ----\")\n",
    "            print(\"Train loss: \")\n",
    "            print(\"Discriminator: \", str(d_x_loss))\n",
    "            print(\"Generator: \", str(g_T_real_loss))\n",
    "            print(\"Last PSNR: \", meanPSNR)\n",
    "            print(\"Last SSIM: \", meanSSIM)\n",
    "            losses.append((d_x_loss.item(), g_T_real_loss.item()))\n",
    "            D_losses.append((D_X_hr_real_loss.item(), D_X_hr_fake_loss.item()))\n",
    "            G_losses.append((D_X_hr_real_fake_loss.item(), MSE_loss.item()))\n",
    "    print(\"Done\")\n",
    "    return losses, D_losses, G_losses, meanPSNR_lst, meanSSIM_lst\n",
    "\n",
    "\n",
    "##########################################################\n",
    "############### Training Multiple models #################\n",
    "\n",
    "def trainModelEnsemble(name=\"name\", n_epochs=100, dim=320, scale_factors = [2, 4, 8], batch_size=16, n_res_blocks=8, GAN_loss_weight=10**-3, GPU=True, IMPORT_GENERATOR=False):\n",
    "    # Records losses of the trained models\n",
    "    loss_dict = {}\n",
    "    for i, scale_factor in enumerate(scale_factors):\n",
    "        # Define data loaders:\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        dataloaders = [train_loader, test_loader]\n",
    "\n",
    "        # Create model:\n",
    "        generator, discriminator, t, t_test = create_model(dim=dim, scale_factor=scale_factor, batch_size=batch_size, n_res_blocks=n_res_blocks, GPU=GPU)\n",
    "        if IMPORT_GENERATOR:\n",
    "            del generator\n",
    "            path = \"checkpoints/SRResNet_\" + str(scale_factor) + \"_generator\"\n",
    "            generator, _ = readModels(path, gpu=GPU)\n",
    "        models = [generator, discriminator]\n",
    "        \n",
    "        # Define optimizers:\n",
    "        g_optimizer = optim.Adam(generator.parameters(), lr, [beta1, beta2])\n",
    "        d_optimizer = optim.Adam(discriminator.parameters(), lr, [beta1, beta2])\n",
    "        optimizers = [g_optimizer, d_optimizer]\n",
    "\n",
    "        print(\"\\n --- Training with parameters: ---\")\n",
    "        print(\"scale factor: \", scale_factor)\n",
    "        print(\"epochs: \", n_epochs)\n",
    "        print(\"batch size: \", batch_size)\n",
    "        \n",
    "        # Train model:\n",
    "        losses, D_losses, G_losses, meanPSNR_lst, meanSSIM_lst = training_loop(models=models,\n",
    "                                                                               optimizers=optimizers,\n",
    "                                                                               dataloaders=dataloaders,\n",
    "                                                                               n_epochs=n_epochs,\n",
    "                                                                               t=t,\n",
    "                                                                               t_test=t_test,\n",
    "                                                                               GAN_loss_weight=GAN_loss_weight,\n",
    "                                                                               GPU=GPU)\n",
    "        loss_dict[name + \"_\" + str(scale_factor)] = (losses, D_losses, G_losses, meanPSNR_lst, meanSSIM_lst)\n",
    "            \n",
    "        # Save trained model:\n",
    "        path = \"checkpoints/\" + name + \"_\" + str(scale_factor)\n",
    "        saveModels(generator, discriminator, path=path)\n",
    "\n",
    "        # Delete the trained models\n",
    "        del generator\n",
    "        del discriminator\n",
    "        print(\"Passed training of model: \", name+str(scale_factor))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE PARAMETERS FOR TRAINING\n",
    "GPU = True\n",
    "IMPORT_GENERATOR = True\n",
    "dim = 96\n",
    "batch_size = 32\n",
    "n_res_blocks = 6\n",
    "n_epochs = 400\n",
    "scale_factors = [2, 4, 8]\n",
    "GAN_loss_weight=10**-3\n",
    "# Parameters of optimizer:\n",
    "lr = 0.0001\n",
    "beta1 = 0.5\n",
    "beta2 = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose augmentations:\n",
    "transforms=[Flip(), Rotate(), ToTensor(), RandomCrop(height=dim, width=dim, always_apply=True)]\n",
    "test_transforms=[ToTensor()]\n",
    "# Create dataset:\n",
    "train_data = MabulaDataset(file_path=\"/Data/OCTA/Train\", transforms=transforms)\n",
    "test_data = MabulaDataset(file_path=\"/Data/OCTA/Test\", transforms=test_transforms)\n",
    "# Create dataloader:\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=12, shuffle=True, num_workers=0)\n",
    "# Clamp dataloaders:\n",
    "dataloaders = [train_loader, test_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to Train SRGAN\n",
    "# Note: Assumes that the SRResNet has been pretrained.\n",
    "#loss_dict = trainModelEnsemble(name=\"SRResNetGAN\",\n",
    "#                               n_epochs=n_epochs,\n",
    "#                               dim=dim,\n",
    "#                               scale_factors=scale_factors,\n",
    "#                               batch_size=batch_size,\n",
    "#                               n_res_blocks=n_res_blocks,\n",
    "#                               GAN_loss_weight=GAN_loss_weight,\n",
    "#                               GPU=GPU,\n",
    "#                               IMPORT_GENERATOR=IMPORT_GENERATOR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
